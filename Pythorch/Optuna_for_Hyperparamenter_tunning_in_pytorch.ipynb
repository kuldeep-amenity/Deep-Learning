{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zav179ztT1Xn",
        "outputId": "297e629c-080c-4e66-d92e-ea9f9e9c7798"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna) (6.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "# Install the Optuna library for hyperparameter optimization\n",
        "!pip install optuna\n",
        "\n",
        "# Import necessary PyTorch modules\n",
        "import torch\n",
        "import torch.nn as nn # Neural network modules (e.g., layers)\n",
        "import torch.optim as optim # Optimization algorithms (e.g., Adam)\n",
        "\n",
        "# Import Optuna for hyperparameter tuning\n",
        "import optuna\n",
        "\n",
        "# Import data utilities from PyTorch\n",
        "from torch.utils.data import DataLoader # For batching and shuffling data\n",
        "\n",
        "# Import image datasets and transformations from torchvision\n",
        "from torchvision import datasets, transforms # For MNIST dataset and data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    # Constructor for the neural network\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Net, self).__init__() # Initialize the base nn.Module class\n",
        "        # Define the first fully connected (linear) layer\n",
        "        # Input size: 28*28 (flattened MNIST image), Output size: hidden_size (tunable)\n",
        "        self.fc1 = nn.Linear(28*28, hidden_size)\n",
        "        # Define the second fully connected (linear) layer\n",
        "        # Input size: hidden_size, Output size: 10 (for 10 MNIST digits)\n",
        "        self.fc2 = nn.Linear(hidden_size, 10)\n",
        "\n",
        "    # Defines the forward pass of the neural network\n",
        "    def forward(self, x):\n",
        "        # Flatten the input tensor (e.g., from (batch_size, 1, 28, 28) to (batch_size, 784))\n",
        "        x = torch.flatten(x, 1)\n",
        "        # Apply the first linear layer followed by a ReLU activation function\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        # Apply the second linear layer (output layer)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Gn93NtcET3Sb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "    # Suggest an integer for the hidden layer size between 128 and 512\n",
        "    hidden_size = trial.suggest_int('hidden_size', 128, 512)\n",
        "    # Suggest a floating-point number for the learning rate between 1e-4 and 1e-1 (log-uniform distribution)\n",
        "    learning_rate = trial.suggest_float('lr', 1e-4, 1e-1, log=True)\n",
        "\n",
        "    # Define image transformations: convert image to PyTorch tensor\n",
        "    transform = transforms.Compose([transforms.ToTensor()])\n",
        "    # Load the MNIST training dataset and create a DataLoader\n",
        "    # Downloads the dataset if not available, shuffles it, and uses a batch size of 32\n",
        "    train_loader = DataLoader(datasets.MNIST(\n",
        "        './data', train=True, download=True, transform=transform), batch_size=32, shuffle=True)\n",
        "\n",
        "    # Initialize the neural network model with the suggested hidden_size\n",
        "    model = Net(hidden_size)\n",
        "    # Initialize the Adam optimizer with the model parameters and suggested learning_rate\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    # Define the loss function: Cross Entropy Loss, suitable for multi-class classification\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "    # Iterate for one epoch (training loop)\n",
        "    for epoch in range(1):\n",
        "        # Iterate through batches of data from the train_loader\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            # Zero out the gradients from the previous iteration\n",
        "            optimizer.zero_grad()\n",
        "            # Perform a forward pass: get predictions from the model\n",
        "            output = model(data)\n",
        "            # Calculate the loss between the predictions and the true targets\n",
        "            loss = criterion(output, target)\n",
        "            # Perform a backward pass: compute gradients of the loss with respect to model parameters\n",
        "            loss.backward()\n",
        "            # Update model parameters using the optimizer\n",
        "            optimizer.step()\n",
        "\n",
        "    # Return the loss value for Optuna to minimize\n",
        "    return loss.item()"
      ],
      "metadata": {
        "id": "t-_NNVaQT-J9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an Optuna study object, aiming to minimize the objective function\n",
        "study = optuna.create_study(direction='minimize')\n",
        "# Run the optimization process, calling the 'objective' function for 5 trials\n",
        "study.optimize(objective, n_trials=5)\n",
        "# Print the best hyperparameters found by Optuna\n",
        "print(\"Best Hyperparameters:\", study.best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3W2MvAZYUJtC",
        "outputId": "2606c334-e8ab-426c-dcf5-0627803b28e9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-12-16 13:24:23,852] A new study created in memory with name: no-name-e76aaa73-09e0-43e0-a2cd-6ce60133e9ec\n",
            "[I 2025-12-16 13:24:40,196] Trial 0 finished with value: 0.5686120390892029 and parameters: {'hidden_size': 448, 'lr': 0.006725217873395584}. Best is trial 0 with value: 0.5686120390892029.\n",
            "[I 2025-12-16 13:24:53,393] Trial 1 finished with value: 0.05871235579252243 and parameters: {'hidden_size': 298, 'lr': 0.0020325105977196797}. Best is trial 1 with value: 0.05871235579252243.\n",
            "[I 2025-12-16 13:25:04,122] Trial 2 finished with value: 0.28575313091278076 and parameters: {'hidden_size': 165, 'lr': 0.0011340906581675688}. Best is trial 1 with value: 0.05871235579252243.\n",
            "[I 2025-12-16 13:25:15,634] Trial 3 finished with value: 0.11740577220916748 and parameters: {'hidden_size': 298, 'lr': 0.00346089699178028}. Best is trial 1 with value: 0.05871235579252243.\n",
            "[I 2025-12-16 13:25:25,646] Trial 4 finished with value: 0.7570409178733826 and parameters: {'hidden_size': 176, 'lr': 0.06752191236699764}. Best is trial 1 with value: 0.05871235579252243.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'hidden_size': 298, 'lr': 0.0020325105977196797}\n"
          ]
        }
      ]
    }
  ]
}