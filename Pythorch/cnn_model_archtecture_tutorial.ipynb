{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciHPYI9CLxO-"
      },
      "outputs": [],
      "source": [
        "import torch # Import the main PyTorch library\n",
        "import torch.nn as nn # Import the neural network module from PyTorch\n",
        "import torch.optim as optim # Import optimization algorithms from PyTorch\n",
        "import torchvision # Import torchvision for datasets and models\n",
        "import torchvision.transforms as transforms # Import image transformations\n",
        "import torch.nn.functional as F # Import functional interface for neural network operations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a sequence of image transformations to be applied to the dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), # Convert images from PIL Image or NumPy ndarray to PyTorch tensors\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) # Normalize the tensor image with mean and standard deviation\n",
        "\n",
        "# Load the CIFAR10 training dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, # Specify the root directory for data, indicate it's the training set\n",
        "                                        download=True, transform=transform) # Download data if not present, apply the defined transformations\n",
        "\n",
        "# Create a DataLoader for the training dataset\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, # Load data in batches of 4 samples\n",
        "                                          shuffle=True, num_workers=2) # Shuffle data at each epoch, use 2 subprocesses for data loading\n",
        "\n",
        "# Load the CIFAR10 test dataset\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, # Specify the root directory for data, indicate it's the test set\n",
        "                                       download=True, transform=transform) # Download data if not present, apply the defined transformations\n",
        "\n",
        "# Create a DataLoader for the test dataset\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4, # Load data in batches of 4 samples\n",
        "                                         shuffle=False, num_workers=2) # Do not shuffle test data, use 2 subprocesses for data loading\n",
        "\n",
        "# Define the class names for the CIFAR10 dataset\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6N7QGNyMCtd",
        "outputId": "48024dcb-89c5-4467-ba55-79d8332ac06d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 47.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Convolutional Neural Network (CNN) architecture\n",
        "class Net(nn.Module): # Inherit from nn.Module, the base class for all neural network modules\n",
        "    def __init__(self): # Constructor for the network\n",
        "        super(Net, self).__init__() # Call the constructor of the parent class (nn.Module)\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5) # First convolutional layer: 3 input channels (for RGB images), 6 output channels, 5x5 kernel size\n",
        "        self.pool = nn.MaxPool2d(2, 2) # Max pooling layer: 2x2 window, stride of 2\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5) # Second convolutional layer: 6 input channels, 16 output channels, 5x5 kernel size\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120) # First fully connected (linear) layer: input features from flattened conv2 output, 120 output features\n",
        "        self.fc2 = nn.Linear(120, 84) # Second fully connected layer: 120 input features, 84 output features\n",
        "        self.fc3 = nn.Linear(84, 10) # Third fully connected layer: 84 input features, 10 output features (for 10 classes)\n",
        "\n",
        "    def forward(self, x): # Defines the forward pass of the network\n",
        "        x = self.pool(F.relu(self.conv1(x))) # Apply conv1, then ReLU activation, then max pooling\n",
        "        x = self.pool(F.relu(self.conv2(x))) # Apply conv2, then ReLU activation, then max pooling\n",
        "        x = x.view(-1, 16 * 5 * 5) # Reshape the tensor for the fully connected layers (flatten operation)\n",
        "        x = F.relu(self.fc1(x)) # Apply fc1, then ReLU activation\n",
        "        x = F.relu(self.fc2(x)) # Apply fc2, then ReLU activation\n",
        "        x = self.fc3(x) # Apply fc3 (output layer)\n",
        "        return x # Return the output predictions\n",
        "\n",
        "\n",
        "net = Net() # Create an instance of the Net class"
      ],
      "metadata": {
        "id": "axpQOIpQMHq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss() # Define the loss function: Cross-Entropy Loss, suitable for multi-class classification\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) # Define the optimizer: Stochastic Gradient Descent (SGD) with learning rate 0.001 and momentum 0.9"
      ],
      "metadata": {
        "id": "uEytJLykMOPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(2): # Loop over the dataset multiple times for a specified number of epochs (here, 2 epochs)\n",
        "\n",
        "    running_loss = 0.0 # Initialize running loss for tracking average loss per mini-batch\n",
        "    for i, data in enumerate(trainloader, 0): # Iterate over batches of data from the training data loader\n",
        "        inputs, labels = data # Get the input images and their corresponding labels from the batch\n",
        "\n",
        "        optimizer.zero_grad() # Zero the gradients of all optimized tensors to prevent accumulation from previous iterations\n",
        "\n",
        "        outputs = net(inputs) # Perform a forward pass: get predictions from the network for the current inputs\n",
        "        loss = criterion(outputs, labels) # Calculate the loss between the network's outputs and the true labels\n",
        "        loss.backward() # Perform a backward pass: compute gradients of the loss with respect to all learnable parameters\n",
        "        optimizer.step() # Update the network's parameters using the calculated gradients\n",
        "\n",
        "        running_loss += loss.item() # Accumulate the loss for the current batch\n",
        "        if i % 2000 == 1999: # Check if 2000 mini-batches have been processed\n",
        "            print('[%d, %5d] loss: %.3f' % # Print the current epoch, batch number, and average loss\n",
        "                  (epoch + 1, i + 1, running_loss / 2000)) # Format and display the information\n",
        "            running_loss = 0.0 # Reset running loss after printing\n",
        "\n",
        "print('Finished Training') # Indicate that the training process has completed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqRgeeQ7MOSW",
        "outputId": "610a3bbe-00b3-406a-dc48-5e4898b781a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,  2000] loss: 2.287\n",
            "[1,  4000] loss: 1.920\n",
            "[1,  6000] loss: 1.693\n",
            "[1,  8000] loss: 1.583\n",
            "[1, 10000] loss: 1.515\n",
            "[1, 12000] loss: 1.464\n",
            "[2,  2000] loss: 1.380\n",
            "[2,  4000] loss: 1.355\n",
            "[2,  6000] loss: 1.355\n",
            "[2,  8000] loss: 1.310\n",
            "[2, 10000] loss: 1.291\n",
            "[2, 12000] loss: 1.284\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0 # Initialize counter for correctly predicted samples\n",
        "total = 0 # Initialize counter for total samples processed\n",
        "with torch.no_grad(): # Disable gradient calculation for inference, saves memory and computation\n",
        "    for data in testloader: # Iterate over batches of data from the test data loader\n",
        "        images, labels = data # Get the input images and their corresponding labels from the batch\n",
        "        outputs = net(images) # Perform a forward pass: get predictions from the network for the test images\n",
        "        _, predicted = torch.max(outputs.data, 1) # Get the index of the class with the highest probability (the prediction)\n",
        "        total += labels.size(0) # Add the number of samples in the current batch to the total count\n",
        "        correct += (predicted == labels).sum().item() # Count how many predictions were correct in the current batch\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % ( # Print the overall accuracy\n",
        "    100 * correct / total)) # Calculate and format the accuracy percentage"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdAdL8AJMOVJ",
        "outputId": "c422c6ad-3d4c-4f8a-974e-78fab2701efe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 55 %\n"
          ]
        }
      ]
    }
  ]
}